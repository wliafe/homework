{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72ca7e-badf-4126-b65d-017a10129cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import mltools\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTMModel(nn.Module):\n",
    "    \"\"\"循环神经网络预测模型\"\"\"\n",
    "\n",
    "    def __init__(self, *args, vocab_size, **kwargs):\n",
    "        \"\"\"初始化函数\"\"\"\n",
    "        nn.Module.__init__(self, *args, **kwargs)\n",
    "        self.vocab_size = vocab_size  # 定义词汇表大小\n",
    "        self.hidden_layer = nn.LSTM(self.vocab_size, hidden_size=256, num_layers=1, batch_first=True)  # 定义隐藏层\n",
    "        self.output_layer = nn.Linear(256, self.vocab_size)  # 定义输出层\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        x = F.one_hot(x, self.vocab_size)  # 将输入嵌入, x形状为(批量大小, 时间步数, 嵌入大小)\n",
    "        x = x.to(torch.float32)\n",
    "        x, state = self.hidden_layer(x, state)  # x形状为(批量大小, 时间步数, 隐藏大小), state形状为(隐藏层数, 批量大小, 隐藏大小)\n",
    "        x = self.output_layer(x)  # 它的输出形状是(批量大小, 时间步数, 输出大小)\n",
    "        x = x.permute(0, 2, 1)  # 交换时间步数和输出大小的维度, x形状为(批量大小, 输出大小, 时间步数)\n",
    "        return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573274ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size=32, num_steps=35)\n",
    "model = LSTMTMModel(vocab_size=len(vocab))  # 定义训练模型\n",
    "model.to(device)\n",
    "loss = nn.CrossEntropyLoss()  # 设置损失函数\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)  # 设置优化器\n",
    "ml = mltools.MachineLearning(\"LSTMTM\")\n",
    "ml.add_model(model)\n",
    "epoch, timer = ml.batch_create(create_recorder=False)\n",
    "recorder = ml.create_recorder(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298c934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "num_epochs = epoch(500)\n",
    "animator = ml.create_animator(xlabel=\"epoch\", xlim=[0, epoch.totol_epoch + 1], ylim=-0.1, legend=[\"train perplexity\"])  # 创建动画器\n",
    "for current_epoch in range(1, num_epochs + 1):\n",
    "    timer.start()\n",
    "\n",
    "    # 计算训练集\n",
    "    metric_train = mltools.Accumulator(2)  # 累加器：(train_loss, train_size)\n",
    "    model.train()  # 训练模式\n",
    "    for x, y in train_iter:\n",
    "        x = x.to(device)  # 转换x\n",
    "        y = y.to(device)  # 转换y\n",
    "        y_train, _ = model(x)  # 计算模型\n",
    "        train_loss = loss(y_train, y)  # 计算训练损失\n",
    "\n",
    "        # 梯度更新\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, norm_type=2)\n",
    "        optimizer.step()\n",
    "\n",
    "        metric_train.add(train_loss * y.numel(), y.numel())\n",
    "    recorder[0].append(math.exp(metric_train[0] / metric_train[1]))\n",
    "\n",
    "    timer.stop()\n",
    "\n",
    "    # 打印输出值\n",
    "    ml.logger.info(f\"train perplexity {recorder[0][-1]:.3f}\")\n",
    "    ml.print_training_time_massage(timer, num_epochs, current_epoch)\n",
    "    ml.logger.info(f\"trained on {str(device)}\")\n",
    "    animator.show(recorder.data)\n",
    "else:\n",
    "    # 打印输出值\n",
    "    ml.logger.info(f\"train perplexity {recorder[0][-1]:.3f}\")\n",
    "    ml.print_training_time_massage(timer, num_epochs, current_epoch)\n",
    "    ml.logger.info(f\"trained on {str(device)}\")\n",
    "    animator.show(recorder.data)\n",
    "ml.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65562a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "model.eval()\n",
    "metric = mltools.Accumulator(2)  # 累加器：(test_acc, test_size)\n",
    "with torch.no_grad():\n",
    "    for x, y in train_iter:\n",
    "        x = x.to(device)  # 转换x\n",
    "        y = y.to(device)  # 转换y\n",
    "        y_test, _ = model(x)  # 计算模型\n",
    "        test_pred = y_test.argmax(dim=1)  # 计算准确率\n",
    "        test_acc = (test_pred == y).sum()  # 计算测试准确率\n",
    "        metric.add(test_acc, y.numel())\n",
    "ml.logger.info(f\"test acc {metric[0] / metric[1]:.3f}\")  # 计算测试准确率并输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模型\n",
    "model.eval()\n",
    "prefix, num_preds = \"time traveller \", 50\n",
    "outputs = vocab[list(prefix)]\n",
    "state = None\n",
    "for y in prefix:  # 预热期\n",
    "    _, state = model(torch.tensor([vocab[y]], device=device).reshape(1, 1), state)\n",
    "for _ in range(num_preds):  # 预测num_preds步\n",
    "    y, state = model(torch.tensor([outputs[-1]], device=device).reshape(1, 1), state)\n",
    "    outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "print(\"\".join([vocab.idx_to_token[i] for i in outputs]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
